---
title: Comparing Least Squares Calculations
abstract: |
  Many statistics methods require one or more least squares problems to
  be solved. There are several ways to perform this calculation, using
  objects from the base R system and using objects in the classes
  defined in the Matrix package.

  We compare the speed of some of these methods on a very small example
  and on a example for which the model matrix is large and sparse.
author: |
  Douglas Bates\
  R Development Core Team\
  [`Douglas.Bates@R-project.org`](mailto:Douglas.Bates@R-project.org)
date: '2024-07-14'
vignette: |-
  %\VignetteEngine{knitr::knitr}
  %\VignetteEncoding{UTF-8}
  %\VignetteIndexEntry{Comparisons of Least Squares calculation speeds}
  %\VignetteDepends{Matrix,datasets,stats,utils}
output:
  bookdown::html_document2:
    base_format: rmarkdown::html_vignette
    number_sections: yes
bibliography: Matrix.bib

---

::: article
# Linear least squares calculations {#sec:LeastSquares}

Many statistical techniques require least squares solutions
$$\label{eq:LeastSquares}
  \widehat{\mathbf{\beta}}=
  \arg\min_{\mathbf{\beta}}\left\|\mathbf{y}-\ensuremath{\mathbf{X}}\mathbf{\beta}\right\|^2   (\#eq:LeastSquares)$$
where $\ensuremath{\mathbf{X}}$ is an $n\times p$ model matrix
($p\leq n$), $\mathbf{y}$ is $n$-dimensional and $\mathbf{\beta}$ is $p$
dimensional. Most statistics texts state that the solution to
(\@ref(eq:LeastSquares)) is
$$\label{eq:XPX}
  \widehat{\mathbf{\beta}}=\left(\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{X}}\right)^{-1}\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\mathbf{y}   (\#eq:XPX)$$
when $\ensuremath{\mathbf{X}}$ has full column rank (i.e. the columns of
$\ensuremath{\mathbf{X}}$ are linearly independent) and all too
frequently it is calculated in exactly this way.

## A small example {#sec:smallLSQ}

As an example, let's create a model matrix, mm, and corresponding
response vector, y, for a simple linear regression model using the
Formaldehyde data.

Using t to evaluate the transpose, solve to take an inverse, and the
%\*% operator for matrix multiplication, we can translate \@ref(eq:XPX)
into the [S]{.sans-serif} language as

On modern computers this calculation is performed so quickly that it
cannot be timed accurately in [R]{.sans-serif} [^1]

and it provides essentially the same results as the standard lm.fit
function that is called by lm.

## A large example {#sec:largeLSQ}

For a large, ill-conditioned least squares problem, such as that
described in @koen:ng:2003, the literal translation of (\@ref(eq:XPX))
does not perform well.

Because the calculation of a "cross-product" matrix, such as
$\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{X}}$
or $\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\mathbf{y}$, is a
common operation in statistics, the crossprod function has been provided
to do this efficiently. In the single argument form crossprod(mm)
calculates
$\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{X}}$,
taking advantage of the symmetry of the product. That is, instead of
calculating the $712^2=506944$ elements of
$\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{X}}$
separately, it only calculates the $(712\cdot
713)/2=253828$ elements in the upper triangle and replicates them in the
lower triangle. Furthermore, there is no need to calculate the inverse
of a matrix explicitly when solving a linear system of equations. When
the two argument form of the solve function is used the linear system
$$\label{eq:LSQsol}
  \left(\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{X}}\right) \widehat{\mathbf{\beta}} = \ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{y}}   (\#eq:LSQsol)$$
is solved directly.

Combining these optimizations we obtain

On this computer (2.0 GHz Pentium-4, 1 GB Memory, Goto's BLAS, in Spring
2004) the crossprod form of the calculation is about four times as fast
as the naive calculation. In fact, the entire crossprod solution is
faster than simply calculating
$\ensuremath{\mathbf{X}}\ensuremath{^\mathsf{T}}\ensuremath{\mathbf{X}}$
the naive way.

Note that in newer versions of [R]{.sans-serif} and the BLAS library (as
of summer 2007), [R]{.sans-serif}'s %\*% is able to detect the many
zeros in mm and shortcut many operations, and is hence much faster for
such a sparse matrix than crossprod which currently does not make use of
such optimizations. This is not the case when [R]{.sans-serif} is linked
against an optimized BLAS library such as GOTO or ATLAS. Also, for fully
dense matrices, crossprod() indeed remains faster (by a factor of two,
typically) independently of the BLAS library:

## Least squares calculations with Matrix classes {#sec:MatrixLSQ}

The crossprod function applied to a single matrix takes advantage of
symmetry when calculating the product but does not retain the
information that the product is symmetric (and positive semidefinite).
As a result the solution of (\@ref(eq:LSQsol)) is performed using
general linear system solver based on an LU decomposition when it would
be faster, and more stable numerically, to use a Cholesky decomposition.
The Cholesky decomposition could be used but it is rather awkward

The Matrix package uses the S4 class system [@R:Chambers:1998] to retain
information on the structure of matrices from the intermediate
calculations. A general matrix in dense storage, created by the Matrix
function, has class \"dgeMatrix\" but its cross-product has class
\"dpoMatrix\". The solve methods for the \"dpoMatrix\" class use the
Cholesky decomposition.

Furthermore, any method that calculates a decomposition or factorization
stores the resulting factorization with the original object so that it
can be reused without recalculation.

The model matrix mm is sparse; that is, most of the elements of mm are
zero. The Matrix package incorporates special methods for sparse
matrices, which produce the fastest results of all.

As with other classes in the Matrix package, the dsCMatrix retains any
factorization that has been calculated although, in this case, the
decomposition is so fast that it is difficult to determine the
difference in the solution times.

## Session Info {#session-info .unnumbered}

# References {#references .unnumbered}
:::

[^1]: From R version 2.2.0, system.time() has default argument gcFirst =
    TRUE which is assumed and relevant for all subsequent timings
